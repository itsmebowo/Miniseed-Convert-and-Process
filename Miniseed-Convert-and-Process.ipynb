{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b48956",
   "metadata": {},
   "source": [
    "# Ekstraktor Waveform MiniSEED\n",
    "\n",
    "## Fungsi\n",
    "Alat ini:\n",
    "1. Membaca file MiniSEED gabungan\n",
    "2. Mengekstrak waveform komponen vertikal\n",
    "3. Menampilkan plot semua stasiun yang tersedia\n",
    "4. Memungkinkan pemilihan stasiun untuk diekstrak\n",
    "\n",
    "## Petunjuk Penggunaan\n",
    "1. Jalankan sel di bawah ini\n",
    "2. Saat diminta, tempelkan path lengkap ke file MiniSEED gabungan Anda\n",
    "   - Contoh: `C:/data/combined_waveforms.mseed`\n",
    "3. Skrip akan menampilkan semua stasiun dengan komponen vertikal\n",
    "4. Pilih stasiun dengan memasukkan nomornya (dimulai dari 0), dipisahkan koma\n",
    "   - Contoh: `0, 2, 5`\n",
    "\n",
    "## Output\n",
    "- Mengembalikan waveform yang terekstrak untuk stasiun terpilih\n",
    "- Menyimpan plot sebagai file DAT di direktori yang sama\n",
    "\n",
    "## Catatan\n",
    "- Hanya memproses komponen vertikal (saluran Z)\n",
    "- Memerlukan library Obspy terinstal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "\n",
    "# Replace with the path to your combined MiniSEED file\n",
    "file_mseed = input(\"Enter the path to the combined MiniSEED file: \")\n",
    "\n",
    "# Read the combined MiniSEED file\n",
    "st = read(file_mseed)\n",
    "\n",
    "# Plot waveforms for vertical components and collect user input\n",
    "vertical_components = []\n",
    "\n",
    "# Determine the number of vertical components\n",
    "for tr in st:\n",
    "    if tr.stats.channel.endswith('Z'):\n",
    "        vertical_components.append(tr)\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "num_plots = len(vertical_components)\n",
    "num_cols = 1  # Single column layout\n",
    "\n",
    "# Create a figure with enough vertical space\n",
    "plt.figure(figsize=(15, 5 * num_plots))\n",
    "\n",
    "for i, tr in enumerate(vertical_components):\n",
    "    # Plot the waveform\n",
    "    plt.subplot(num_plots, num_cols, i + 1)\n",
    "    time = tr.times()  # Time in seconds\n",
    "    amplitude = tr.data  # Amplitude data\n",
    "    plt.plot(time, amplitude, color=\"b\", lw=1)\n",
    "    plt.xlabel(\"Time (s)\", fontsize=10)\n",
    "    plt.ylabel(\"Amplitude\", fontsize=10)\n",
    "    plt.title(f\"Station: {tr.stats.station}, Channel: {tr.stats.channel}\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.subplots_adjust(hspace=0.5)  # Increase vertical space between plots\n",
    "plt.show()\n",
    "\n",
    "# Ask the user which vertical components to save\n",
    "indices_to_save = input(\"Enter the indices of the vertical components you want to save (comma-separated, e.g., 0,2): \")\n",
    "indices_to_save = [int(index.strip()) for index in indices_to_save.split(\",\")]\n",
    "\n",
    "# Save the selected vertical components\n",
    "for index in indices_to_save:\n",
    "    tr = vertical_components[index]\n",
    "    filename = f\"{tr.stats.station}_vertical.dat\"\n",
    "    time = tr.times()  # Time in seconds\n",
    "    amplitude = tr.data  # Amplitude data\n",
    "\n",
    "    # Save the data in .dat format\n",
    "    np.savetxt(filename, np.column_stack((time, amplitude,amplitude)), fmt=\"%.6f %6f %6f\")\n",
    "    print(f\"File saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efed62",
   "metadata": {},
   "source": [
    "# Alat Pemberian Nama Event\n",
    "\n",
    "## Tujuan\n",
    "Menambahkan nama event sebagai prefix pada file .dat untuk:\n",
    "- Organisasi file yang lebih baik\n",
    "- Penelusuran data\n",
    "- Manajemen dataset\n",
    "\n",
    "## Kapan Digunakan\n",
    "*Setelah* menyelesaikan phase/group velocity picking pada file .dat\n",
    "\n",
    "## Petunjuk Penggunaan\n",
    "1. Jalankan sel di bawah ini\n",
    "2. Saat diminta:\n",
    "   - Masukkan path direktori berisi file .dat\n",
    "     - Contoh: `C:/processed_data/event_123/`\n",
    "   - Masukkan prefix yang diinginkan (nama event diikuti '-')\n",
    "     - Contoh: `Java-` atau `Sumatra-2023-`\n",
    "3. Skrip akan:\n",
    "   - Menampilkan semua file .dat\n",
    "   - Mengubah nama dengan prefix\n",
    "   - Menunjukkan nama sebelum/sesudah\n",
    "\n",
    "## Contoh Perubahan\n",
    "Sebelum:\n",
    "CDisp.T.AAK_vertical.dat\n",
    "\n",
    "Sesudah (prefix `Java-`):\n",
    "Java-CDisp.T.AAK_vertical.dat\n",
    "\n",
    "\n",
    "## Catatan\n",
    "- Hanya memodifikasi file .dat\n",
    "- File asli akan ditimpa (buat backup terlebih dahulu)\n",
    "- Berfungsi untuk semua file dalam direktori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7512eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: Minahasa-CDisp.T.kwjn_PaluEQ_2.dat -> Java-Minahasa-CDisp.T.kwjn_PaluEQ_2.dat\n",
      "Renamed: Minahasa-CDisp.T.lsa_PaluEQ_2.dat -> Java-Minahasa-CDisp.T.lsa_PaluEQ_2.dat\n",
      "Renamed: Minahasa-CDisp.T.majo_PaluEQ_2.dat -> Java-Minahasa-CDisp.T.majo_PaluEQ_2.dat\n",
      "Renamed: Minahasa-CDisp.T.mbwa_PaluEQ_2.dat -> Java-Minahasa-CDisp.T.mbwa_PaluEQ_2.dat\n",
      "Renamed: Minahasa-CDisp.T.mdj_PaluEQ_2.dat -> Java-Minahasa-CDisp.T.mdj_PaluEQ_2.dat\n",
      "Renamed: Minahasa-GDisp.kwjn_PaluEQ_2.dat -> Java-Minahasa-GDisp.kwjn_PaluEQ_2.dat\n",
      "Renamed: Minahasa-GDisp.lsa_PaluEQ_2.dat -> Java-Minahasa-GDisp.lsa_PaluEQ_2.dat\n",
      "Renamed: Minahasa-GDisp.majo_PaluEQ_2.dat -> Java-Minahasa-GDisp.majo_PaluEQ_2.dat\n",
      "Renamed: Minahasa-GDisp.mbwa_PaluEQ_2.dat -> Java-Minahasa-GDisp.mbwa_PaluEQ_2.dat\n",
      "Renamed: Minahasa-GDisp.mdj_PaluEQ_2.dat -> Java-Minahasa-GDisp.mdj_PaluEQ_2.dat\n",
      "Prefix addition complete!\n"
     ]
    }
   ],
   "source": [
    "# Add prefix to filename\n",
    "import os\n",
    "\n",
    "def add_prefix_to_dat_files(directory, prefix):\n",
    "    \"\"\"\n",
    "    Adds a prefix to all .dat files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .dat files\n",
    "        prefix (str): The prefix to add to each filename\n",
    "    \"\"\"\n",
    "    # Get all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a .dat file\n",
    "        if filename.endswith('.dat'):\n",
    "            # Construct new filename with prefix\n",
    "            new_filename = prefix + filename\n",
    "            old_path = os.path.join(directory, filename)\n",
    "            new_path = os.path.join(directory, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input for directory and prefix\n",
    "    target_directory = input(\"Enter the directory path containing .dat files: \")\n",
    "    prefix_to_add = input(\"Enter the prefix to add to filenames: \")\n",
    "    \n",
    "    # Verify directory exists\n",
    "    if not os.path.isdir(target_directory):\n",
    "        print(f\"Error: Directory '{target_directory}' does not exist.\")\n",
    "    else:\n",
    "        add_prefix_to_dat_files(target_directory, prefix_to_add)\n",
    "        print(\"Prefix addition complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d2076",
   "metadata": {},
   "source": [
    "# Penyalin Header\n",
    "\n",
    "## Tujuan\n",
    "Menyalin informasi header (metadata) antar file data seismik ketika:\n",
    "- Setup MATLAB gagal mentransfer koordinat\n",
    "- Perlu sinkronisasi manual metadata stasiun/event  \n",
    "- Pemulihan dari kesalahan proses data\n",
    "\n",
    "## Data yang Disalin\n",
    "- Koordinat sumber gempa\n",
    "- Koordinat stasiun rekaman\n",
    "- Elevasi stasiun\n",
    "- Kedalaman gempa\n",
    "- Metadata header lainnya\n",
    "\n",
    "## Petunjuk Penggunaan  \n",
    "1. Jalankan sel di bawah ini\n",
    "2. Saat diminta:\n",
    "   - **Path Sumber**: Direktori dengan file header yang benar\n",
    "     - Contoh: `C:/correct_headers/`\n",
    "   - **Path Target**: Direktori yang perlu pembaruan header\n",
    "     - Contoh: `C:/processing_output/`\n",
    "    - **Backup**: Akan diminta apakah ingin melakukan backup dari data yang ada di path target, ketik 'y' jika ya dan 'n' jika tidak\n",
    "\n",
    "## Logika Pencocokan\n",
    "File dipasangkan menggunakan identifier stasiun:\n",
    "Sumber: INCN_vertical.dat → cocok dengan → Target: Java-GDisp.INCN_vertical.dat\n",
    "\n",
    "\n",
    "## Fitur Keamanan\n",
    "- Menunjukkan pratinjau perubahan\n",
    "- Mempertahankan timestamp asli file\n",
    "- Hanya memodifikasi header (bukan data waveform)\n",
    "\n",
    "## Kasus Penggunaan Disarankan\n",
    "- Mengoreksi kesalahan koordinat setelah proses\n",
    "- Menggabungkan dataset dari sumber berbeda\n",
    "- Pemeriksaan kontrol kualitas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e8fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAT File Header Updater ===\n",
      "Handles both 3-letter and 4-letter identifiers (ABC_vertical, ABCD_vertical)\n",
      "Source files: ABC_vertical.dat or ABCD_vertical.dat\n",
      "Target files: Java-GDisp.ABC_vertical.dat or Java-GDisp.ABCD_vertical.dat\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Created backup of 10 files in C:\\Users\\Bowo\\OneDrive\\Kuliah\\Semester 8\\Gelombang Permukaan\\Tubes\\EGFAnalysisTimeFreq_version_2015\\Minahasa Bowo\\backup_20250425_161047\n",
      "Found source: bjt_PaluEQ_2.dat → Identifier: bjt_PaluEQ\n",
      "Found source: chto_PaluEQ_2.dat → Identifier: chto_PaluEQ\n",
      "Found source: coco_PaluEQ_2.dat → Identifier: coco_PaluEQ\n",
      "Found source: ctao_PaluEQ_2.dat → Identifier: ctao_PaluEQ\n",
      "Found source: dav_PaluEQ_2.dat → Identifier: dav_PaluEQ\n",
      "Found source: dgar_PaluEQ_2.dat → Identifier: dgar_PaluEQ\n",
      "Found source: enh_PaluEQ_2.dat → Identifier: enh_PaluEQ\n",
      "Found source: erm_PaluEQ_2.dat → Identifier: erm_PaluEQ\n",
      "Found source: gumo_PaluEQ_2.dat → Identifier: gumo_PaluEQ\n",
      "Found source: hia_PaluEQ_2.dat → Identifier: hia_PaluEQ\n",
      "Found source: hkps_PaluEQ_2.dat → Identifier: hkps_PaluEQ\n",
      "Found source: hnr_PaluEQ_2.dat → Identifier: hnr_PaluEQ\n",
      "Found source: incn_PaluEQ_2.dat → Identifier: incn_PaluEQ\n",
      "Found source: kapi_PaluEQ_2.dat → Identifier: kapi_PaluEQ\n",
      "Found source: kmi_PaluEQ_2.dat → Identifier: kmi_PaluEQ\n",
      "Found source: kwjn_PaluEQ_2.dat → Identifier: kwjn_PaluEQ\n",
      "Found source: lsa_PaluEQ_2.dat → Identifier: lsa_PaluEQ\n",
      "Found source: majo_PaluEQ_2.dat → Identifier: majo_PaluEQ\n",
      "Found source: mbwa_PaluEQ_2.dat → Identifier: mbwa_PaluEQ\n",
      "Found source: mdj_PaluEQ_2.dat → Identifier: mdj_PaluEQ\n",
      "Found source: nwao_PaluEQ_2.dat → Identifier: nwao_PaluEQ\n",
      "Found source: palk_PaluEQ_2.dat → Identifier: palk_PaluEQ\n",
      "Found source: pmg_PaluEQ_2.dat → Identifier: pmg_PaluEQ\n",
      "Found source: qiz_PaluEQ_2.dat → Identifier: qiz_PaluEQ\n",
      "Found source: sse_PaluEQ_2.dat → Identifier: sse_PaluEQ\n",
      "Found source: tato_PaluEQ_2.dat → Identifier: tato_PaluEQ\n",
      "Found source: tau_PaluEQ_2.dat → Identifier: tau_PaluEQ\n",
      "Found source: uln_PaluEQ_2.dat → Identifier: uln_PaluEQ\n",
      "Found source: wake_PaluEQ_2.dat → Identifier: wake_PaluEQ\n",
      "Found source: wrab_PaluEQ_2.dat → Identifier: wrab_PaluEQ\n",
      "Found source: xan_PaluEQ_2.dat → Identifier: xan_PaluEQ\n",
      "✅ Updated Minahasa-CDisp.T.kwjn_PaluEQ_2.dat with header from kwjn_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-CDisp.T.lsa_PaluEQ_2.dat with header from lsa_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-CDisp.T.majo_PaluEQ_2.dat with header from majo_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-CDisp.T.mbwa_PaluEQ_2.dat with header from mbwa_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-CDisp.T.mdj_PaluEQ_2.dat with header from mdj_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-GDisp.kwjn_PaluEQ_2.dat with header from kwjn_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-GDisp.lsa_PaluEQ_2.dat with header from lsa_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-GDisp.majo_PaluEQ_2.dat with header from majo_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-GDisp.mbwa_PaluEQ_2.dat with header from mbwa_PaluEQ_2.dat\n",
      "✅ Updated Minahasa-GDisp.mdj_PaluEQ_2.dat with header from mdj_PaluEQ_2.dat\n",
      "\n",
      "📊 Summary:\n",
      "- Found 31 valid source files\n",
      "- Processed 10 target files\n",
      "- Updated 10 files\n",
      "- 0 files unchanged\n",
      "\n",
      "🎉 Operation complete. Modified 10 target files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_identifier(filename):\n",
    "    \"\"\"Extract the XXX_vertical or XXXX_vertical portion from filenames\"\"\"\n",
    "    # Matches patterns like:\n",
    "    # Source: ABC_vertical.dat → \"ABC_vertical\"\n",
    "    # Source: ABCD_vertical.dat → \"ABCD_vertical\"\n",
    "    # Target: Java-GDisp.ABC_vertical.dat → \"ABC_vertical\"\n",
    "    # Target: Java-GDisp.ABCD_vertical.dat → \"ABCD_vertical\"\n",
    "    match = re.search(r'([a-z]+_[A-Za-z0-9]+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def copy_first_two_lines(source_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Copies first two lines from source .dat files to matching target .dat files.\n",
    "    Handles both 3-letter and 4-letter identifiers (ABC_vertical, ABCD_vertical).\n",
    "    \"\"\"\n",
    "    # Create mapping of identifiers to source files\n",
    "    source_map = {}\n",
    "    source_count = 0\n",
    "    \n",
    "    # Process source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.dat'):\n",
    "            identifier = extract_identifier(filename)\n",
    "            if identifier:\n",
    "                source_map[identifier] = os.path.join(source_dir, filename)\n",
    "                source_count += 1\n",
    "                print(f\"Found source: {filename} → Identifier: {identifier}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping source file (pattern mismatch): {filename}\")\n",
    "    \n",
    "    if not source_map:\n",
    "        print(\"❌ No valid source files found!\")\n",
    "        return 0\n",
    "    \n",
    "    # Process target directory\n",
    "    modified_count = 0\n",
    "    target_count = 0\n",
    "    \n",
    "    for filename in os.listdir(target_dir):\n",
    "        if filename.endswith('.dat'):\n",
    "            target_count += 1\n",
    "            identifier = extract_identifier(filename)\n",
    "            if identifier:\n",
    "                if identifier in source_map:\n",
    "                    source_path = source_map[identifier]\n",
    "                    target_path = os.path.join(target_dir, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        # Read source lines\n",
    "                        with open(source_path, 'r') as src_file:\n",
    "                            new_lines = [src_file.readline(), src_file.readline()]\n",
    "                        \n",
    "                        # Read target content\n",
    "                        with open(target_path, 'r') as tgt_file:\n",
    "                            existing_lines = tgt_file.readlines()[2:]\n",
    "                        \n",
    "                        # Write combined content\n",
    "                        with open(target_path, 'w') as tgt_file:\n",
    "                            tgt_file.writelines(new_lines + existing_lines)\n",
    "                        \n",
    "                        print(f\"✅ Updated {filename} with header from {os.path.basename(source_path)}\")\n",
    "                        modified_count += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error processing {filename}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ No matching source for target: {filename} (identifier: {identifier})\")\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping target file (pattern mismatch): {filename}\")\n",
    "    \n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"- Found {source_count} valid source files\")\n",
    "    print(f\"- Processed {target_count} target files\")\n",
    "    print(f\"- Updated {modified_count} files\")\n",
    "    print(f\"- {target_count - modified_count} files unchanged\")\n",
    "    return modified_count\n",
    "\n",
    "def create_backup(target_dir):\n",
    "    \"\"\"Create timestamped backup of target files\"\"\"\n",
    "    backup_dir = os.path.join(target_dir, f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    backup_count = 0\n",
    "    for filename in os.listdir(target_dir):\n",
    "        if filename.endswith('.dat'):\n",
    "            try:\n",
    "                src = os.path.join(target_dir, filename)\n",
    "                dst = os.path.join(backup_dir, filename)\n",
    "                with open(src, 'rb') as f1, open(dst, 'wb') as f2:\n",
    "                    f2.write(f1.read())\n",
    "                backup_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to backup {filename}: {str(e)}\")\n",
    "    \n",
    "    print(f\"📦 Created backup of {backup_count} files in {backup_dir}\")\n",
    "    return backup_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== DAT File Header Updater ===\")\n",
    "    print(\"Handles both 3-letter and 4-letter identifiers (ABC_vertical, ABCD_vertical)\")\n",
    "    print(\"Source files: ABC_vertical.dat or ABCD_vertical.dat\")\n",
    "    print(\"Target files: Java-GDisp.ABC_vertical.dat or Java-GDisp.ABCD_vertical.dat\\n\")\n",
    "    \n",
    "    source_dir = input(\"📂 Source directory path: \").strip()\n",
    "    target_dir = input(\"📂 Target directory path: \").strip()\n",
    "    \n",
    "    if not all(map(os.path.isdir, [source_dir, target_dir])):\n",
    "        print(\"❌ Error: Invalid directory path(s)\")\n",
    "        exit(1)\n",
    "    \n",
    "    if input(\"💾 Create backup before modifying? (y/n): \").lower() == 'y':\n",
    "        create_backup(target_dir)\n",
    "    \n",
    "    modified = copy_first_two_lines(source_dir, target_dir)\n",
    "    print(f\"\\n🎉 Operation complete. Modified {modified} target files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d5e29",
   "metadata": {},
   "source": [
    "# Dokumentasi Pembersih Data QC untuk Hasil Picking Seismik\n",
    "\n",
    "## Fungsi Utama\n",
    "Skrip ini dirancang untuk melakukan Quality Control (QC) otomatis pada file `.dat` setelah proses picking fase/grup velocity di MATLAB dengan:\n",
    "\n",
    "1. **Pembersihan Nilai Nol**:\n",
    "   - Menghapus baris yang mengandung nilai nol atau mendekati nol\n",
    "   - Presisi deteksi bisa disesuaikan (default: `1e-6`)\n",
    "\n",
    "2. **Penyederhanaan Kolom**:\n",
    "   - Menghapus kolom ke-3 dan ke-4 secara otomatis\n",
    "   - Mempertahankan kolom penting untuk analisis tomografi\n",
    "\n",
    "3. **Proteksi Data**:\n",
    "   - Tidak mengubah 2 baris pertama (header informasi)\n",
    "   - Opsi backup data asli sebelum pemrosesan\n",
    "\n",
    "## Cara Penggunaan\n",
    "\n",
    "### Langkah 1: Menjalankan Skrip\n",
    "1. Buka Jupyter Notebook yang berisi skrip ini\n",
    "2. Jalankan cell yang berisi kode QC cleaner\n",
    "\n",
    "### Langkah 2: Input Parameter\n",
    "1. **Path Direktori**:\n",
    "   - Masukkan alamat folder tempat file `.dat` disimpan\n",
    "   - Contoh: `C:/data/hasil_picking/` atau `/home/user/seismic_data`\n",
    "\n",
    "2. **Opsi Backup**:\n",
    "   - Ketik `y` jika ingin membuat salinan backup\n",
    "   - Ketik `n` jika tidak membutuhkan backup\n",
    "\n",
    "### Langkah 3: Proses Otomatis\n",
    "Skrip akan:\n",
    "1. Memindai semua file `.dat` di direktori\n",
    "2. Menampilkan log pemrosesan\n",
    "3. Menyimpan hasil bersih di direktori yang sama\n",
    "\n",
    "## Fitur Keamanan\n",
    "- **Backup Otomatis**:\n",
    "  - Data asli disalin ke folder `backup_[tanggal]`\n",
    "  - Format: `backup_20240515` (tahun-bulan-tanggal)\n",
    "\n",
    "- **Proteksi Header**:\n",
    "  - Baris 1-2 (informasi koordinat dan parameter) tidak diubah\n",
    "  - Hanya memodifikasi bagian data waveform\n",
    "\n",
    "- **Threshold Disesuaikan**:\n",
    "  - Nilai ambang nol bisa diubah sesuai kebutuhan\n",
    "  - Untuk data sangat presisi, bisa diperketat menjadi `1e-8`\n",
    "\n",
    "## Output yang Dihasilkan\n",
    "- File dengan format nama sama\n",
    "- Struktur file:\n",
    "[Baris 1-2: Header asli]\n",
    "[Baris 3+: Data bersih tanpa kolom 3-4 dan nilai nol]\n",
    "\n",
    "- Log terminal akan menampilkan:\n",
    "[Nama file]: 15 baris nol dihapus\n",
    "[Nama file]: Kolom 3-4 dihapus\n",
    "\n",
    "\n",
    "## Catatan Penting\n",
    "1. Pastikan file sudah selesai diproses di MATLAB sebelum di-QC\n",
    "2. Untuk data dengan format khusus, sesuaikan threshold nilai nol\n",
    "3. Backup sangat disarankan untuk menghindari kehilangan data\n",
    "4. Skrip hanya bekerja untuk file `.dat` dengan struktur kolom standar\n",
    "\n",
    "## Contoh Kasus Penggunaan\n",
    "1. Koreksi otomatis setelah picking rapat gelombang\n",
    "2. Pembersihan data sebelum inversi tomografi\n",
    "3. Standarisasi format file untuk analisis batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56b351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAT File Processor ===\n",
      "1. Preserves first two lines\n",
      "2. Removes 3rd and 4th columns\n",
      "3. Deletes lines with any zero values (0, 0.0, 0.00, etc.)\n",
      "\n",
      "📦 Created backup of 10 files in C:\\Users\\Bowo\\OneDrive\\Kuliah\\Semester 8\\Gelombang Permukaan\\Tubes\\EGFAnalysisTimeFreq_version_2015\\Minahasa Bowo\\backup_20250425_161117\n",
      "✅ Processed Minahasa-CDisp.T.kwjn_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-CDisp.T.lsa_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-CDisp.T.majo_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-CDisp.T.mbwa_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-CDisp.T.mdj_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-GDisp.kwjn_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-GDisp.lsa_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-GDisp.majo_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-GDisp.mbwa_PaluEQ_2.dat (removed 46 lines)\n",
      "✅ Processed Minahasa-GDisp.mdj_PaluEQ_2.dat (removed 46 lines)\n",
      "\n",
      "🎉 Operation complete:\n",
      "- Processed 10 files\n",
      "- Removed 460 lines containing zero values\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def is_approximately_zero(value):\n",
    "    \"\"\"Check if a string value represents approximately zero\"\"\"\n",
    "    try:\n",
    "        float_val = float(value)\n",
    "        return abs(float_val) < 1e-6  # Adjust threshold as needed\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def remove_columns_and_zero_lines(filepath):\n",
    "    \"\"\"\n",
    "    Process a .dat file to:\n",
    "    1. Keep first two lines intact\n",
    "    2. Remove 3rd and 4th columns from remaining lines\n",
    "    3. Delete lines containing any approximately zero values in remaining columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Keep first two lines intact\n",
    "        header = lines[:2]\n",
    "        \n",
    "        # Process remaining lines\n",
    "        processed_lines = []\n",
    "        lines_removed = 0\n",
    "        \n",
    "        for line in lines[2:]:\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "                \n",
    "            # Split on whitespace (handles both tabs and spaces)\n",
    "            columns = re.split(r'\\s+', line)\n",
    "            \n",
    "            # Remove columns 3 and 4 (if they exist)\n",
    "            if len(columns) >= 4:\n",
    "                columns = columns[:2] + columns[4:]\n",
    "            \n",
    "            # Check for any zero values in the remaining columns\n",
    "            has_zero = any(is_approximately_zero(val) for val in columns)\n",
    "            if has_zero:\n",
    "                lines_removed += 1\n",
    "                continue  # Skip the line if any remaining column has zero\n",
    "            \n",
    "            # Add the processed line to the result\n",
    "            processed_lines.append(' '.join(columns) + '\\n')\n",
    "        \n",
    "        # Write back to file\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.writelines(header + processed_lines)\n",
    "        \n",
    "        return True, lines_removed\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {os.path.basename(filepath)}: {str(e)}\")\n",
    "        return False, 0\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"Process all .dat files in a directory\"\"\"\n",
    "    processed_count = 0\n",
    "    total_lines_removed = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.dat'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            success, lines_removed = remove_columns_and_zero_lines(filepath)\n",
    "            if success:\n",
    "                print(f\"✅ Processed {filename} (removed {lines_removed} lines)\")\n",
    "                processed_count += 1\n",
    "                total_lines_removed += lines_removed\n",
    "    \n",
    "    return processed_count, total_lines_removed\n",
    "\n",
    "def create_backup(directory):\n",
    "    \"\"\"Create timestamped backup of files\"\"\"\n",
    "    backup_dir = os.path.join(directory, f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    backup_count = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.dat'):\n",
    "            try:\n",
    "                src = os.path.join(directory, filename)\n",
    "                dst = os.path.join(backup_dir, filename)\n",
    "                with open(src, 'rb') as f1, open(dst, 'wb') as f2:\n",
    "                    f2.write(f1.read())\n",
    "                backup_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to backup {filename}: {str(e)}\")\n",
    "    \n",
    "    print(f\"📦 Created backup of {backup_count} files in {backup_dir}\")\n",
    "    return backup_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== DAT File Processor ===\")\n",
    "    print(\"1. Preserves first two lines\")\n",
    "    print(\"2. Removes 3rd and 4th columns\")\n",
    "    print(\"3. Deletes lines with any zero values (0, 0.0, 0.00, etc.)\\n\")\n",
    "    \n",
    "    target_dir = input(\"📂 Directory containing .dat files: \").strip()\n",
    "    \n",
    "    if not os.path.isdir(target_dir):\n",
    "        print(\"❌ Error: Invalid directory path\")\n",
    "        exit(1)\n",
    "    \n",
    "    if input(\"💾 Create backup before modifying? (y/n): \").lower() == 'y':\n",
    "        create_backup(target_dir)\n",
    "    \n",
    "    processed, total_removed = process_directory(target_dir)\n",
    "    print(f\"\\n🎉 Operation complete:\")\n",
    "    print(f\"- Processed {processed} files\")\n",
    "    print(f\"- Removed {total_removed} lines containing zero values\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
